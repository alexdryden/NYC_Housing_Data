{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "# Reproducible Notebook (Draft)\n",
    "## University of Illinois\n",
    "### Open Data Mashups\n",
    "### Alex Dryden\n",
    "### Spring 2019\n",
    "\n",
    "<div>\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents: \n",
    "\n",
    "### [Purpose](#Purpose)\n",
    "\n",
    "### [Methods](#Methods)\n",
    "- [Loading Data Files](#Load)\n",
    "- [Filtering Data](#Selecting-Data)\n",
    "- [Assembly (Rinse, Repeat)](#Rinse,-Repeat)\n",
    "- [Output](#Write)\n",
    "\n",
    "### [Executive Summary](#Executive-Summary)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Purpose\n",
    "\n",
    "This notebook should allow anyone interested in the final dataset to confirm how it was created from the [intermediate](./intermediate_index.ipynb) data. The methods section is designed to walk someone with minimal Python experience through the I/O, filtering, concatonation of the final dataset. See the [Executive Summary](#Executive-Summary) section for a more compressed version.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "## Methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div align=right>[Table of Contents](#Table-of-Contents)</div>\n",
    "\n",
    "\n",
    "### Loading Data Files\n",
    "\n",
    "So long as you are running this as a jupyter notebook, the current working directory is the same directory with the intermediate data files. If you are not, change the directory variable to the correct path where you have stored the intermediate data files. \n",
    "\n",
    "All of the intermediate data has been saved as a CSV, but we will be filtering and combining that data using a popular data manipulation and analysis library, pandas. Pansas has a simple way to import CSV data. By convention, we import the pandas library aliased as 'pd'.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#file location\n",
    "directory = './intermediate_data/intermediate_csv/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<div align=right>[Table of Contents](#Table-of-Contents)</div>\n",
    "\n",
    "\n",
    "### Managing Memory\n",
    "\n",
    "Pandas is good at reducing the amount of RAM required to store and manipulate data, especially for numeric data. However, our intermediate datasets, when stored as a pandas dataframe, range from a few hundred MB to several GB. On an older laptop with other programs running in the background, it might cause some problems if we are doing all of our work while holding all of the intermediate data in memory. So we will work with and filter them one at a time, using the Jupyter reset magic to help manage memeory. \n",
    "\n",
    "Because this dataset is fouced only on regulated buildings that were sold 2018, we will start with that dataset and use it to as the primary filter. We eliminated columns that we did not need in the [intermediate](./intermediate_index.ipynb) data cleaning phase. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['price',\n",
       " 'cap_rate',\n",
       " 'borough_cap_rate',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'BIN',\n",
       " 'deed_date',\n",
       " 'watchlist']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#generate a dataframe from the csv file and use the BBL column as the index\n",
    "transactions_df = pd.read_csv(directory + 'TransactionsClean.csv', index_col='BBL')\n",
    "\n",
    "#with any pandas DF, using Python's built-in list() function will generate a list \n",
    "#of the column names. We can visually inspect that we have the columns we need\n",
    "list(transactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<div align=right>[Table of Contents](#Table-of-Contents)</div>\n",
    "\n",
    "\n",
    "### Filtering Data\n",
    "\n",
    "We are using the BBL building identifier as an index becuase it is a common language that we can translate other building identifiers into, which we did during cleaning. Now we have a convenient way to quickly and effeciently filter the remaining data. Pandas offers a database-style join opperation that allows you to combine data with Venn Diagram-like logic. \n",
    "\n",
    "In this case, we are going to keep <i>all</i> of the data from the transactions data and add to it <i>any</i> rows from the new dataset that share the same BBL as an existing row in the transactions data. This has the effect of adding additional columns to our transactions dataset. If a row in the transactions dataframe has no match in the columns being added, then a Null value will populate that field. We will do this with each of the remaining intermediate datasets.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['price',\n",
       " 'cap_rate',\n",
       " 'borough_cap_rate',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'BIN',\n",
       " 'deed_date',\n",
       " 'watchlist',\n",
       " 'Owner',\n",
       " 'Total_Units',\n",
       " 'Residential_Units',\n",
       " 'Buildings',\n",
       " 'Stories',\n",
       " 'Land_Market_Value',\n",
       " 'Total_Market_Value',\n",
       " 'Transitional_Assesed_Land_Value',\n",
       " 'Transitional_Assessed_Total_Value,',\n",
       " 'Actual_Assesed_Land_Value',\n",
       " 'Actual_Assessed_Total_Value,',\n",
       " '5_Year_Exemption_Flags',\n",
       " 'Exempt_Total_Value']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate a dataframe from the csv file and use the BBL column as the indes\n",
    "valuation_df = pd.read_csv(directory + 'DoFValuationClean.csv', index_col='BBL')\n",
    "\n",
    "#left join the transaction data with the boiler data, default is to join index-to-index\n",
    "final_dataset = transactions_df.join(other=valuation_df, how='left')\n",
    "\n",
    "#inspect to make sure all required columns are present \n",
    "list(final_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div align=right>[Table of Contents](#Table-of-Contents)</div>\n",
    "\n",
    "\n",
    "### <i> Manage Memory </i>\n",
    "These datasets are orders of magnatude smaller now than when they were coverted into intermediate datasets. But on lightweight machines it may still be worthwile to remove pointers to memory we no longer need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['directory', 'final_dataset', 'pd', 'transactions_df', 'valuation_df']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%who_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now lets remove the boiler_df and confirm that it is no longer in the namespace\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['directory', 'final_dataset', 'pd']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use reset magic to remove the df from memory\n",
    "\n",
    "%reset_selective -f transactions_df\n",
    "%reset_selective -f valuation_df\n",
    "\n",
    "%who_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=right>[Table of Contents](#Table-of-Contents)</div>\n",
    "\n",
    "\n",
    "\n",
    "### Rinse, Repeat\n",
    "\n",
    "Because we have BBL as a column in each of our intermediate datasets, we can easily use the same process we outlined above for the remaing intermediate datasets. To simplify the code, we store theremaining intermediate datasets in a list and process them in a loop.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remaining 5 dataset names\n",
    "intermediate_data = [\n",
    "    'HDPComplaintProblemsClean.csv',\n",
    "    'DHPFeesClean.csv',\n",
    "]\n",
    "\n",
    "#for each dataset, load it into a df, set index, join to transactions and then remove\n",
    "for dataset in intermediate_data:\n",
    "    other = pd.read_csv(directory + dataset, index_col='BBL')\n",
    "    final_dataset.join(other=other, how='left')\n",
    "    %reset_selective -f other\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=right>[Table of Contents](#Table-of-Contents)</div>\n",
    "\n",
    "\n",
    "### Output\n",
    "\n",
    "Now all that is left is to write the data to a CSV. To demonstrate that this has totally recreated the final dataset, we can use checksums to compare them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Checksum\n",
    "We can use the md5 algorithm to get a hash value of the final dataset you downloaded with this directory and then delete that dataset. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a27d32de6bdf518332c54306e27d140c'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hashlib\n",
    "import os\n",
    "\n",
    "loc = 'final_dataset.csv'\n",
    "\n",
    "with open(loc, 'rb') as f:\n",
    "    data = f.read()\n",
    "    first_checksum = hashlib.md5(data).hexdigest()\n",
    "    \n",
    "os.remove(loc)\n",
    "first_checksum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<div align=right>[Table of Contents](#Table-of-Contents)</div>\n",
    "\n",
    "\n",
    "\n",
    "#### Write\n",
    "Now we write the final dataset we just created to the same location\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.to_csv(r'./final_dataset.csv', index='BBL', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Compare\n",
    "\n",
    "Now we get the checksum of the csv we just produced and compare it to the previous checksum\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(loc, 'rb') as f:\n",
    "    data = f.read()\n",
    "    second_checksum = hashlib.md5(data).hexdigest()\n",
    "    \n",
    "first_checksum == second_checksum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The two values are the same, so we know we just reproduced the final dataset exactly\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
